# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1750DqCOU9t97Vc9Yuoy923nlIF-lg5SC
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Load the Titanic dataset
titanic_data = pd.read_csv('titanic.csv')  # Assuming 'titanic.csv' is in your Colab environment

# Display some basic information
titanic_data.head()
titanic_data.info()
titanic_data.describe()

# Correlation matrix
# Select only numerical features for correlation analysis
numerical_features = titanic_data.select_dtypes(include=np.number)
correlation_matrix = numerical_features.corr()

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Visualize distributions and relationships
sns.pairplot(titanic_data, hue='Survived')  # Explore relationships between variables
plt.show()

sns.boxplot(x='Pclass', y='Age', data=titanic_data)  # Example to check outliers
plt.title('Age distribution by Passenger Class')
plt.show()

# Handle missing values (example: fill missing 'Age' with the mean)
titanic_data['Age'].fillna(titanic_data['Age'].mean(), inplace=True)

# Convert categorical features to numerical using one-hot encoding
titanic_data = pd.get_dummies(titanic_data, columns=['Sex', 'Embarked'], drop_first=True)

from sklearn.feature_selection import SelectKBest, chi2

# Separate features (X) and target (y)
X = titanic_data[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S']]
y = titanic_data['Survived']

# Select the top k features using chi-squared test (example: k=5)
selector = SelectKBest(chi2, k=5)
X_new = selector.fit_transform(X, y)

# Get the selected feature names
selected_features = X.columns[selector.get_support(indices=True)]
print(selected_features)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression  # Example: Logistic Regression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)

# Create and train the model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)